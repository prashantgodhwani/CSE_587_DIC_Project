{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "NGVwR-exCrfl",
      "metadata": {
        "id": "NGVwR-exCrfl"
      },
      "source": [
        "*Inspired* and repurposed code by [ Lingjun (Ivan) Chen and Nuo (Nora) Xu[link text](https://)](https://sites.northwestern.edu/msia/2019/04/24/personalized-restaurant-recommender-system-using-hybrid-approach/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f5ylAOK-cS0h",
      "metadata": {
        "id": "f5ylAOK-cS0h"
      },
      "source": [
        "## Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54283bb5",
      "metadata": {
        "id": "54283bb5"
      },
      "outputs": [],
      "source": [
        "# importing required libraries\n",
        "import pandas as pd\n",
        "from collections import Counter \n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from lightfm import LightFM\n",
        "from lightfm.evaluation import precision_at_k,auc_score,reciprocal_rank\n",
        "import scipy\n",
        "import time\n",
        "import math\n",
        "from lightfm.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "azqJKF0ophor",
      "metadata": {
        "id": "azqJKF0ophor"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cb92f69",
      "metadata": {
        "id": "5cb92f69"
      },
      "outputs": [],
      "source": [
        "# loading data from CSV\n",
        "business=pd.read_csv('drive/MyDrive/business_final.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00dda267",
      "metadata": {
        "id": "00dda267"
      },
      "outputs": [],
      "source": [
        "rest_review=pd.read_csv('drive/MyDrive/rest_review.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8djpPXvffYc7",
      "metadata": {
        "id": "8djpPXvffYc7"
      },
      "source": [
        "## Fetching the categories of business"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ktwfdp9dBHC6",
      "metadata": {
        "id": "Ktwfdp9dBHC6"
      },
      "outputs": [],
      "source": [
        "# Fetching the categories of business\n",
        "category_frequency = [content for line in business['categories'] for content in line.split(\";\")]\n",
        "num_tags = [len(content) for line in business['categories'] for content in line.split(\";\")]\n",
        "sum(num_tags)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "rAPuBBhUXRUs",
      "metadata": {
        "id": "rAPuBBhUXRUs"
      },
      "source": [
        "## Taking the top 60 categories as our features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tf8HyfNqg9R-",
      "metadata": {
        "id": "tf8HyfNqg9R-"
      },
      "outputs": [],
      "source": [
        "# using counter tp count the frequency of common categories\n",
        "new_feature = Counter(category_frequency).most_common(60)\n",
        "print(new_feature)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "oDhPPU39Xa-T",
      "metadata": {
        "id": "oDhPPU39Xa-T"
      },
      "source": [
        "## One hot encoding with chosen features and merging them with dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cwyELMdyCGGB",
      "metadata": {
        "id": "cwyELMdyCGGB"
      },
      "outputs": [],
      "source": [
        "new_feature = Counter(category_frequency).most_common(60)# Getting top 60 most frequent categories\n",
        "\n",
        "feature = pd.DataFrame()\n",
        "for ind, val in enumerate(new_feature[1:], start=1):\n",
        "  category, freq = val[0], val[1]\n",
        "  idf_score = math.log1p(len(business.business_id) / freq) # Calculating IDF score\n",
        "  feature.loc[ind-1, ['Feature', 'Category_Score']] = category, idf_score # Adding category and its IDF score to the dataframe 'feature'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XbEtg_OUDHiF",
      "metadata": {
        "id": "XbEtg_OUDHiF"
      },
      "outputs": [],
      "source": [
        "business2 = business\n",
        "\n",
        "for index in range(feature.shape[0]):\n",
        "    # Get the feature name and IDF score from the current row in 'feature'\n",
        "    f,idf = feature.loc[index,'Feature'],feature.loc[index,'Category_Score']\n",
        "    # For each row in the 'categories' column in 'business2', create a binary feature \n",
        "    # indicating whether or not the current feature is present in the list of categories\n",
        "    # for that row, and then multiply the result by the IDF score for that feature\n",
        "    business2[f] = [1 / len(each_line) * idf if f in each_line else 0 \n",
        "                      for each_line in business2['categories'].str.split(';')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7bd1ee3",
      "metadata": {
        "id": "c7bd1ee3"
      },
      "outputs": [],
      "source": [
        "rest_review.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ec9db5",
      "metadata": {
        "id": "28ec9db5"
      },
      "outputs": [],
      "source": [
        "business2.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce2feeb2",
      "metadata": {
        "id": "ce2feeb2"
      },
      "outputs": [],
      "source": [
        "rest_review=rest_review.merge(business2,on='business_id',how='inner')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e577b4",
      "metadata": {
        "id": "89e577b4"
      },
      "outputs": [],
      "source": [
        "rest_review=rest_review.drop('Unnamed: 0_x',axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "nKWoW6jcX0Dv",
      "metadata": {
        "id": "nKWoW6jcX0Dv"
      },
      "source": [
        "## Loading user dataset and extracting features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0eda434",
      "metadata": {
        "id": "a0eda434"
      },
      "outputs": [],
      "source": [
        "# Reading and selecting columns from yelp_user.csv file\n",
        "user=pd.read_csv('drive/MyDrive/yelp_user.csv')\n",
        "user=user[['user_id','review_count','useful']]\n",
        "# Renaming columns to avoid confusion later\n",
        "user=user.rename(columns={'review_count':'user_rc','useful':'user_useful'})\n",
        "rest_review=rest_review.merge(user,on='user_id',how='inner')\n",
        "rest_review.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77766c3c",
      "metadata": {
        "id": "77766c3c"
      },
      "outputs": [],
      "source": [
        "rest_review.columns# num_tags"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "SRzTz241YN6G",
      "metadata": {
        "id": "SRzTz241YN6G"
      },
      "source": [
        "## Normalizing columns "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84eeff3b",
      "metadata": {
        "id": "84eeff3b"
      },
      "outputs": [],
      "source": [
        "# normalizing review count and useful columns\n",
        "rest_review.review_count = pd.Series([math.sqrt(x) for x in rest_review.review_count])\n",
        "rest_review.useful =  pd.Series([math.sqrt(x) for x in rest_review.useful])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JjRgBrAU6yEa",
      "metadata": {
        "id": "JjRgBrAU6yEa"
      },
      "outputs": [],
      "source": [
        "rest_review=rest_review.sample(frac=.01)\n",
        "business_id=rest_review.business_id.unique()\n",
        "business2=business2[business2['business_id'].isin(business_id)]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "H9aUHE6AYfFB",
      "metadata": {
        "id": "H9aUHE6AYfFB"
      },
      "source": [
        "## Created object dataset and fit it with user_id and business_id "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BFJGPG9CYdzZ",
      "metadata": {
        "id": "BFJGPG9CYdzZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c53a34c1",
      "metadata": {
        "id": "c53a34c1"
      },
      "outputs": [],
      "source": [
        "#  Fit dataset on user and business data\n",
        "data_set = Dataset()\n",
        "data_set.fit(rest_review.user_id,business2.business_id)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "F2H3t7rTZhbP",
      "metadata": {
        "id": "F2H3t7rTZhbP"
      },
      "source": [
        "## Fitting other features such as stars, review_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a89fe92e",
      "metadata": {
        "id": "a89fe92e"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_set.fit_partial(items=business2.business_id,\n",
        "                    item_features=['stars','review_count'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b723c4f6",
      "metadata": {
        "id": "b723c4f6"
      },
      "outputs": [],
      "source": [
        "# creating a list of features from business table that include only categories we split and fitting them to our data_set.\n",
        "item_cols = [x for x in business2.columns[21:]]\n",
        "data_set.fit_partial(items = business2.business_id,item_features = item_cols)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b3fe65f",
      "metadata": {
        "id": "9b3fe65f"
      },
      "outputs": [],
      "source": [
        "item_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3762005b",
      "metadata": {
        "id": "3762005b"
      },
      "outputs": [],
      "source": [
        "rest_review.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abaa5f3a",
      "metadata": {
        "id": "abaa5f3a"
      },
      "outputs": [],
      "source": [
        "# creating list of features from rest_review table and fitting them to the model\n",
        "user_1 = [x for x in rest_review.columns[29:]]\n",
        "user_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b19e33f3",
      "metadata": {
        "id": "b19e33f3"
      },
      "outputs": [],
      "source": [
        "data_set.fit_partial(users=rest_review.user_id,\n",
        "                    user_features = user_1)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "v4iFQcb8Zy5Y",
      "metadata": {
        "id": "v4iFQcb8Zy5Y"
      },
      "source": [
        "## Created interaction sparse matrix from review table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a384ab5",
      "metadata": {
        "id": "9a384ab5"
      },
      "outputs": [],
      "source": [
        "# creating interactions\n",
        "(interactions, weights) = data_set.build_interactions([(x['user_id'],x['business_id'],x[['stars_x']]) for index,x in rest_review.iterrows()])\n",
        "print(repr(interactions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32663d1e",
      "metadata": {
        "id": "32663d1e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52099601",
      "metadata": {
        "id": "52099601"
      },
      "outputs": [],
      "source": [
        "def compute_rmse(X_test, X_pred):\n",
        "    # Ref: https://github.com/ncu-dart/rdf/blob/master/rdf/utils.py\n",
        "    \n",
        "    sse = 0.\n",
        "    for i in range(len(X_test)):\n",
        "        sse += (X_test[i] - X_pred[i]) ** 2\n",
        "    \n",
        "    return (sse / len(X_test)) ** .5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "BNBPkdkGZ8jH",
      "metadata": {
        "id": "BNBPkdkGZ8jH"
      },
      "source": [
        "created functions that bulids dictionaries which are used to user and item feature mapping and the values for the features are normalized to preserve the importance of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YAKc3ajSZ7N6",
      "metadata": {
        "id": "YAKc3ajSZ7N6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1724d99f",
      "metadata": {
        "id": "1724d99f"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "# Build item features function\n",
        "def item(df,item_cols,values):\n",
        "    output = {}\n",
        "    for col in item_cols:\n",
        "        output.update({col: df[col]})\n",
        "    sum_val = sum(float(value) for value in output.values()) # get sum of all the tfidf values\n",
        "    \n",
        "    if(sum_val == 0):\n",
        "        return output\n",
        "    else:\n",
        "      for key, value in output.items():\n",
        "        output[key] = value / sum_val # normalizing it to preserve the importance of all features\n",
        "    return output\n",
        "\n",
        "# Build user features function\n",
        "def user_dict(df,item_cols,values):\n",
        "    output = {}\n",
        "    for col in item_cols:\n",
        "        output.update({col: df[col]})\n",
        "    sum_val = sum(list(output.values())) # get sum of all the tfidf values\n",
        "    \n",
        "    if(sum_val == 0):\n",
        "        return output\n",
        "    else:\n",
        "        for key, value in output.items():\n",
        "          output[key] = value / sum_val # normalizing it to preserve the importance of all features\n",
        "    return output\n",
        "\n",
        "# get max of each column to regularize value to [0,1]\n",
        "star_max = business2.stars.max()\n",
        "max_item = business2.review_count.max()\n",
        "max_u_rc = rest_review.review_count.max()\n",
        "max_useful = rest_review.useful.max()\n",
        "\n",
        "# build item features\n",
        "item_features_data = []\n",
        "for index, x in business2.iterrows():\n",
        "    feature_dict = {\n",
        "        'stars': 0.8 * x['stars'] / star_max,\n",
        "        'review_count': 0.2 * x['review_count'] / max_item,\n",
        "    }\n",
        "    feature_dict.update(item(x, item_cols, [0.5 * x['stars'] / star_max, 0.5 * x['review_count'] / max_item]))\n",
        "    item_features_data.append((x['business_id'], feature_dict))\n",
        "item_features = data_set.build_item_features(tqdm(item_features_data, desc=\"Building item features\"))\n",
        "\n",
        "# build user features\n",
        "user_features_data = []\n",
        "for index, x in rest_review.iterrows():\n",
        "    feature_dict = {\n",
        "        'user_rc': 0.7 * x['user_rc'] / max_u_rc,\n",
        "        'user_useful': 0.3 * x['user_useful'] / max_useful,\n",
        "    }\n",
        "    feature_dict.update(user_dict(x, user_1, [0.7 * x['user_rc'] / max_u_rc, 0.3 * x['user_useful'] / max_useful]))\n",
        "    user_features_data.append((x['user_id'], feature_dict))\n",
        "user_features = data_set.build_user_features(tqdm(user_features_data, desc=\"Building user features\"))\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Kio4n9dSaYFf",
      "metadata": {
        "id": "Kio4n9dSaYFf"
      },
      "source": [
        "split out interactions matrix created earlier into train and test with train having 80% and test set having 20%. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b72826e",
      "metadata": {
        "id": "7b72826e"
      },
      "outputs": [],
      "source": [
        "seed = 69\n",
        "from lightfm.cross_validation import random_train_test_split\n",
        "train,test=random_train_test_split(interactions,test_percentage=0.2,random_state=np.random.RandomState(seed))\n",
        "test = test - train.multiply(test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "NrjWO12jcH4y",
      "metadata": {
        "id": "NrjWO12jcH4y"
      },
      "source": [
        "defined hyperparameters and applied grid search to our model to get the best parameters which get the best auc score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RL0wFLQEH_ih",
      "metadata": {
        "id": "RL0wFLQEH_ih"
      },
      "outputs": [],
      "source": [
        "# Grid search to find the best hyperparameters for LightFM model\n",
        "from itertools import product\n",
        "# Define hyperparameter combinations to try\n",
        "NUM_THREADS = [5]\n",
        "NUM_COMPONENTS = [30, 50]\n",
        "LEARNING_RATE = [0.01,  0.1]\n",
        "NUM_EPOCHS = [10, 20]\n",
        "ITEM_ALPHA = [1e-5, 1e-6]\n",
        "loss=['warp','logistic']\n",
        "grid = product(NUM_THREADS, NUM_COMPONENTS, LEARNING_RATE, NUM_EPOCHS, ITEM_ALPHA,loss)\n",
        "# Function to train and evaluate the model using the given hyperparameters \n",
        "def train_evaluate_model(train, test, user_features, item_features, num_threads, num_components, learning_rate, num_epochs, item_alpha,loss):\n",
        "    model = LightFM(loss=loss, item_alpha=item_alpha, random_state=seed, no_components=num_components,learning_rate=learning_rate)\n",
        "    model = model.fit(train, user_features=user_features, item_features=item_features, epochs=num_epochs, num_threads=num_threads)\n",
        "    test_precision = precision_at_k(model, test, train_interactions=train, item_features=item_features, user_features=user_features, k=5, num_threads=num_threads).mean()\n",
        "    test_auc = auc_score(model, test, item_features=item_features, user_features=user_features, num_threads=num_threads).mean()\n",
        "    return (num_threads, num_components, learning_rate, num_epochs, item_alpha, test_precision, test_auc,loss)\n",
        "\n",
        "results = []\n",
        "# Loop through hyperparameters and run the model using each combination\n",
        "for params in grid:\n",
        "    num_threads, num_components, learning_rate, num_epochs, item_alpha,loss = params\n",
        "    result = train_evaluate_model(train, test, user_features, item_features, num_threads, num_components, learning_rate, num_epochs, item_alpha,loss)\n",
        "    results.append(result)\n",
        "    print(f\"num_threads={num_threads}, num_components={num_components}, learning_rate={learning_rate}, num_epochs={num_epochs}, item_alpha={item_alpha}, test_precision={result[5]:.4f}, test_auc={result[6]:.4f},best_func={loss}\")\n",
        "\n",
        "# Find the best result\n",
        "best_result = max(results, key=lambda x: x[6])\n",
        "print(f\"\\nBest hyperparameters: num_threads={best_result[0]}, num_components={best_result[1]}, learning_rate={best_result[2]}, num_epochs={best_result[3]}, item_alpha={best_result[4]}, test_precision={best_result[5]:.4f}, test_auc={best_result[6]:.4f},best_loss_func={best_result[7]}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "-T7XHtOMckRy",
      "metadata": {
        "id": "-T7XHtOMckRy"
      },
      "source": [
        "## Using our best hyperparameters got from grid search "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UXFh6I2moeeW",
      "metadata": {
        "id": "UXFh6I2moeeW"
      },
      "outputs": [],
      "source": [
        "from lightfm.evaluation import  recall_at_k\n",
        "\n",
        "# Define hyperparameters\n",
        "num_threads=5\n",
        "num_components=50\n",
        "learning_rate=0.1\n",
        "num_epochs=[5,10,15,20,25,30]\n",
        "item_alpha=1e-05\n",
        "# Initialize a dataframe to store the test scores\n",
        "test_scores=pd.DataFrame(columns=['AUC','precision','recall'])\n",
        "# Loop through different values of num_epochs\n",
        "for i in num_epochs:\n",
        "  # Train a LightFM model with the specified hyperparameters\n",
        "  model = LightFM(loss='warp',item_alpha=item_alpha,random_state=69,no_components=num_components,learning_rate=learning_rate)\n",
        "  model = model.fit(train,user_features=user_features,item_features=item_features,epochs=i,num_threads=num_threads)\n",
        "  # Compute the AUC, precision@1, and recall@1 scores on the test set\n",
        "  test_auc = auc_score(model, test,user_features=user_features,item_features=item_features,num_threads=num_threads).mean()\n",
        "  precision=precision_at_k(model, test,train_interactions=train,item_features=item_features,user_features=user_features, k=1,num_threads=num_threads).mean()\n",
        "\n",
        "  recall = recall_at_k(model, test,train_interactions=train,\n",
        "                            item_features=item_features, user_features=user_features, \n",
        "                            k=1, num_threads=num_threads).mean()\n",
        "  # Store the scores in the dataframe\n",
        "  test_scores.loc[f'{i}']=[test_auc,precision,recall]\n",
        "  \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "HfU7ou1gO9v3",
      "metadata": {
        "id": "HfU7ou1gO9v3"
      },
      "source": [
        "### Calculating auc score, precision and recall for every epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5oqmmFekTKxs",
      "metadata": {
        "id": "5oqmmFekTKxs"
      },
      "outputs": [],
      "source": [
        "test_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p0BWRma3DXNg",
      "metadata": {
        "id": "p0BWRma3DXNg"
      },
      "outputs": [],
      "source": [
        "def recommend(model, train, data_meta, user_ids, k, name, mapping, tag=None, user_features=None, item_features=None, num_threads=2):\n",
        "    # model: LightFM model object\n",
        "    # train: sparse matrix representing user-item interactions\n",
        "    # data_meta: dataframe containing metadata for items\n",
        "    # user_ids: list of user ids for which recommendations are to be generated\n",
        "    # k: number of items to be recommended per user\n",
        "    # name: column name in data_meta representing item names\n",
        "    # mapping: dictionary mapping item IDs to indices\n",
        "    # tag: (optional) column name in data_meta representing item tags\n",
        "    # user_features: (optional) matrix representing user features\n",
        "    # item_features: (optional) matrix representing item features\n",
        "    # num_threads: number of threads to use for prediction\n",
        "    n_users, n_items = train.shape\n",
        "\n",
        "    recommendations = {}\n",
        "\n",
        "    for user_id in user_ids:\n",
        "        # get indices of items already interacted with by user\n",
        "        t_idx = {value: key for key, value in mapping.items()}\n",
        "        # get names of already interacted items\n",
        "        u_idx = train.getrow(user_id).indices\n",
        "\n",
        "        known_positives = data_meta.loc[u_idx, name]\n",
        "        # if tag is provided, get tags of already interacted items\n",
        "        if tag is not None:\n",
        "            known_tags = data_meta.loc[u_idx, tag]\n",
        "        # predict scores for all items\n",
        "\n",
        "        scores = model.predict(user_id, np.arange(n_items), user_features=user_features, item_features=item_features, num_threads=num_threads)\n",
        "        # get indices of top k items with highest scores\n",
        "        item_ids = np.argsort(-scores)[:k]\n",
        "        # get names of top k items\n",
        "        top_items = data_meta.iloc[item_ids]['name']\n",
        "        # add recommendations for current user to recommendations dictionary\n",
        "        recommendations[user_id] = top_items\n",
        "\n",
        "    return recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1rlRZzKDc_-",
      "metadata": {
        "id": "f1rlRZzKDc_-"
      },
      "outputs": [],
      "source": [
        "user_index=list(set(rf.get_user_index(test)))\n",
        "a=recommend(model,train,business2,[49],5,'name',mapping=data_set.mapping()[2],tag='categories',\n",
        "                              user_features = user_features,item_features=item_features).values()\n",
        "pd.DataFrame(a).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y-bQxjECq6Dz",
      "metadata": {
        "id": "y-bQxjECq6Dz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(test_scores['AUC'])\n",
        "plt.title('AUC')\n",
        "plt.xlabel('Auc')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gJ0HbUh1tXBp",
      "metadata": {
        "id": "gJ0HbUh1tXBp"
      },
      "outputs": [],
      "source": [
        "plt.plot(test_scores['precision'])\n",
        "plt.title('precision')\n",
        "plt.xlabel('precision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nHyjdgv8tf5X",
      "metadata": {
        "id": "nHyjdgv8tf5X"
      },
      "outputs": [],
      "source": [
        "plt.plot(test_scores['recall'])\n",
        "plt.title('recall')\n",
        "plt.xlabel('recall')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P9q_Zvp8o5ss",
      "metadata": {
        "id": "P9q_Zvp8o5ss"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JXVC3Fy72EJ0",
      "metadata": {
        "id": "JXVC3Fy72EJ0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'test_precision': [0.0036, 0.0003, 0.0036, 0.0003, 0.0033, 0.0003, 0.0037, 0.0003, 0.0070, 0.0003, 0.0071, 0.0003, 0.0106, 0.0003, 0.0120, 0.0003, 0.0037, 0.0003, 0.0037, 0.0003, 0.0039, 0.0003, 0.0040, 0.0003],\n",
        "    'test_auc': [0.6398, 0.5757, 0.6391, 0.5757, 0.7286, 0.5755, 0.9276, 0.9755, 0.7737, 0.5750, 0.7788, 0.9751, 0.8041, 0.9749, 0.9069, 0.9751, 0.9327, 0.5759, 0.9321, 0.5759, 0.9242, 0.5756, 0.6223, 0.5756],\n",
        "    'loss_func': ['warp', 'logistic', 'warp', 'logistic', 'warp', 'logistic', 'warp', 'logistic', 'warp', 'logistic', 'warp', 'logistic', 'warp', 'logistic', 'warp', 'logistic', 'warp', 'logistic', 'warp', 'logistic', 'warp', 'logistic', 'warp', 'logistic']\n",
        "})\n",
        "\n",
        "heatmap_data = pd.pivot_table(data, values=['test_precision', 'test_auc'], index='loss_func')\n",
        "\n",
        "sns.heatmap(heatmap_data, cmap='YlGnBu', annot=True, annot_kws={\"size\": 10})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8n2evre0E4m",
      "metadata": {
        "id": "b8n2evre0E4m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KVQCFn5v9Hwi",
      "metadata": {
        "id": "KVQCFn5v9Hwi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
